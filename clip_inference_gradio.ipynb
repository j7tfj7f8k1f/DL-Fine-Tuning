{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/j7tfj7f8k1f/DL-Fine-Tuning/blob/main/clip_inference_gradio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 前置"
      ],
      "metadata": {
        "id": "FVMeX8_zaaA2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b8y-BOB5i55r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21b9417a-b683-4774-d49e-fcc507e06fa6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwHOf3uOjTIt",
        "outputId": "ca0706eb-8cf0-4baf-93a9-9c87e0f29deb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.5.0+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.20.0+cu121)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (5.5.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.16.1)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.10.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.26.4)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (10.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.26.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.115.4)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.4.0)\n",
            "Requirement already satisfied: gradio-client==1.4.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.4.2)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.2)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.10)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart==0.0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.12)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.7.2)\n",
            "Requirement already satisfied: safehttpx<1.0,>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.1.1)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.41.2)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.32.0)\n",
            "Requirement already satisfied: websockets<13.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.4.2->gradio) (12.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.6)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision transformers gradio\n",
        "!transformers-cli env"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C0yyLFeYw1bT"
      },
      "source": [
        "# CLIP Gradio使用 - 文搜圖/ 圖搜文"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 862
        },
        "id": "ZiIm1KI-w4Gx",
        "outputId": "a8691471-4dc2-477d-c6f8-2bc82c0cb3f3"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-8-d92903ac87eb>:14: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"/content/drive/MyDrive/chiikawa/model/best_clip_model.pth\", map_location=device))\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://a0c594662477a008f4.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div><iframe src=\"https://a0c594662477a008f4.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/image_processing_utils.py:41: UserWarning: The following named arguments are not valid for `ChineseCLIPImageProcessor.preprocess` and were ignored: 'padding'\n",
            "  return self.preprocess(images, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/image_processing_utils.py:41: UserWarning: The following named arguments are not valid for `ChineseCLIPImageProcessor.preprocess` and were ignored: 'padding'\n",
            "  return self.preprocess(images, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/image_processing_utils.py:41: UserWarning: The following named arguments are not valid for `ChineseCLIPImageProcessor.preprocess` and were ignored: 'padding'\n",
            "  return self.preprocess(images, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/image_processing_utils.py:41: UserWarning: The following named arguments are not valid for `ChineseCLIPImageProcessor.preprocess` and were ignored: 'padding'\n",
            "  return self.preprocess(images, **kwargs)\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/image_processing_utils.py:41: UserWarning: The following named arguments are not valid for `ChineseCLIPImageProcessor.preprocess` and were ignored: 'padding'\n",
            "  return self.preprocess(images, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import ChineseCLIPProcessor, ChineseCLIPModel\n",
        "import gradio as gr\n",
        "import os\n",
        "import base64\n",
        "from io import BytesIO\n",
        "\n",
        "# 設置設備\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# 載入模型和處理器\n",
        "model = ChineseCLIPModel.from_pretrained(\"OFA-Sys/chinese-clip-vit-base-patch16\")\n",
        "model.load_state_dict(torch.load(\"/content/drive/MyDrive/best_clip_model.pth\", map_location=device))\n",
        "model = model.to(device)\n",
        "model.eval()\n",
        "\n",
        "processor = ChineseCLIPProcessor.from_pretrained(\"OFA-Sys/chinese-clip-vit-base-patch16\")\n",
        "\n",
        "# 圖鑑資料夾路徑\n",
        "image_dir = \"/content/drive/MyDrive\"\n",
        "output_image_path = \"/content/drive/MyDrive\"  # 定義輸出圖片的路徑\n",
        "\n",
        "# 加載中文詞彙表\n",
        "with open('/content/drive/MyDrive/word_list.txt', 'r', encoding='utf-8') as f:\n",
        "    vocab = [line.strip() for line in f.readlines()]\n",
        "\n",
        "\n",
        "# 找跟圖最相似的商品名\n",
        "def find_similar_words(image,top_k=3):\n",
        "    try:\n",
        "        # 確保圖片格式為 RGB\n",
        "        if image.mode != \"RGB\":\n",
        "            image = image.convert(\"RGB\")\n",
        "\n",
        "        batch_size = 16  # 每次處理16個詞彙\n",
        "        similarities = []\n",
        "\n",
        "        # 釋放未使用的顯存\n",
        "        if device == \"cuda\":\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        # 推理並進行相似度計算\n",
        "        with torch.no_grad():\n",
        "            for i in range(0, len(vocab), batch_size):\n",
        "                batch_vocab = vocab[i:i + batch_size]\n",
        "                inputs = processor(\n",
        "                    text=batch_vocab,\n",
        "                    images=image,\n",
        "                    return_tensors=\"pt\",\n",
        "                    padding=True\n",
        "                )\n",
        "                # 確保輸入移動到 GPU\n",
        "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "                # 確保模型和張量在 GPU 上進行推理\n",
        "                outputs = model(**inputs)\n",
        "                image_embeds = outputs.image_embeds / outputs.image_embeds.norm(dim=-1, keepdim=True)\n",
        "                text_embeds = outputs.text_embeds / outputs.text_embeds.norm(dim=-1, keepdim=True)\n",
        "                similarity = torch.matmul(image_embeds, text_embeds.T).squeeze(0).to(device)\n",
        "                similarities.append(similarity)\n",
        "\n",
        "        # 合併所有相似度\n",
        "        similarity = torch.cat(similarities, dim=0)\n",
        "\n",
        "        # 找到相似度最高的詞彙\n",
        "        top_k_indices = torch.topk(similarity, top_k).indices.tolist()\n",
        "        top_k_words = [vocab[idx] for idx in top_k_indices]\n",
        "        result = \"\\n\".join([f\"Top {i+1}: {word}\" for i, word in enumerate(top_k_words)])\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"推理過程中發生錯誤: {str(e)}\"\n",
        "\n",
        "# 搜索圖片函數\n",
        "def find_image_for_word(word):\n",
        "    try:\n",
        "        # 釋放未使用的顯存\n",
        "        if device == \"cuda\":\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "        # 確保文字在 GPU 上進行推理\n",
        "        with torch.no_grad():\n",
        "            text_inputs = processor(\n",
        "                text=[word],\n",
        "                return_tensors=\"pt\",\n",
        "                padding=True\n",
        "            ).to(device)\n",
        "\n",
        "            text_embeds = model.get_text_features(**text_inputs)\n",
        "            text_embeds = text_embeds / text_embeds.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        highest_similarity = -1\n",
        "        best_match_image_path = None\n",
        "\n",
        "        # 遍歷資料夾內所有圖片\n",
        "        for root, _, files in os.walk(image_dir):\n",
        "            for file in files:\n",
        "                if file.endswith(('.png', '.jpg', '.jpeg')):  # 支援常見圖片格式\n",
        "                    image_path = os.path.join(root, file)\n",
        "                    image = Image.open(image_path).convert(\"RGB\")\n",
        "\n",
        "                    # 將圖片移動到 GPU 並取得圖片特徵\n",
        "                    with torch.no_grad():\n",
        "                        image_inputs = processor(\n",
        "                            images=image,\n",
        "                            return_tensors=\"pt\"\n",
        "                        ).to(device)\n",
        "\n",
        "                        image_embeds = model.get_image_features(**image_inputs)\n",
        "                        image_embeds = image_embeds / image_embeds.norm(dim=-1, keepdim=True)\n",
        "\n",
        "                    # 計算相似度\n",
        "                    similarity = torch.matmul(text_embeds, image_embeds.T).item()\n",
        "\n",
        "                    # 更新相似度最高的圖片\n",
        "                    if similarity > highest_similarity:\n",
        "                        highest_similarity = similarity\n",
        "                        best_match_image_path = image_path\n",
        "\n",
        "        # 返回最相似的圖片\n",
        "        if best_match_image_path:\n",
        "            # 將圖片另存為 PNG 或 JPEG 格式\n",
        "            best_image = Image.open(best_match_image_path).convert(\"RGB\")\n",
        "            best_image.save(output_image_path, format=\"PNG\")  # 指定保存為 PNG\n",
        "\n",
        "            # 將圖片轉換為 Base64 格式\n",
        "            buffered = BytesIO()\n",
        "            best_image.save(buffered, format=\"PNG\")\n",
        "            img_base64 = base64.b64encode(buffered.getvalue()).decode()\n",
        "\n",
        "            return output_image_path, f\"匹配圖片：{os.path.basename(best_match_image_path)}，相似度：{highest_similarity:.2f}\", img_base64\n",
        "        else:\n",
        "            return None, \"找不到匹配的圖片\", None\n",
        "\n",
        "    except Exception as e:\n",
        "        return None, f\"搜尋過程中發生錯誤: {str(e)}\", None\n",
        "\n",
        "# # 設置 Gradio 介面\n",
        "# iface = gr.Interface(\n",
        "#     fn=find_image_for_word,\n",
        "#     inputs=[gr.Textbox(label=\"輸入詞彙\")],\n",
        "#     outputs=[gr.Image(type=\"filepath\"), gr.Textbox(), gr.Textbox(label=\"Base64 編碼\")],\n",
        "#     title=\"CLIP 中文模型 - 詞彙與圖像匹配\",\n",
        "#     description=\"輸入一個詞彙，模型會從圖鑑中找到與詞彙最相關的圖片\"\n",
        "# )\n",
        "\n",
        "# # 啟動共享連結\n",
        "# iface.launch(share=True, debug=True)\n",
        "\n",
        "def launch_interface():\n",
        "    with gr.Blocks() as demo:\n",
        "        with gr.Row():\n",
        "            with gr.Column():\n",
        "                function_choice = gr.Dropdown([\"find_similar_words\", \"find_image_for_word\"], label=\"選擇功能\")\n",
        "                input_image = gr.Image(type=\"pil\", label=\"上傳圖片\")\n",
        "                input_word = gr.Textbox(label=\"輸入詞彙\", visible=False)\n",
        "                top_k = gr.Slider(1, 10, value=3, label=\"Top K\", visible=False)\n",
        "                run_button = gr.Button(\"執行\")\n",
        "\n",
        "            with gr.Column():\n",
        "                output_text = gr.Textbox(label=\"結果\")\n",
        "                output_image = gr.Image(type=\"filepath\", label=\"匹配圖片\", visible=False)\n",
        "                output_base64 = gr.Textbox(label=\"Base64 編碼\", visible=False)\n",
        "\n",
        "        # 根據下拉選單選擇顯示/隱藏輸入和輸出組件\n",
        "        def update_inputs_outputs(choice):\n",
        "            if choice == \"find_similar_words\":\n",
        "                return gr.update(visible=True), gr.update(visible=False), gr.update(visible=True), gr.update(visible=True), gr.update(visible=False), gr.update(visible=False)\n",
        "            else:\n",
        "                return gr.update(visible=False), gr.update(visible=True), gr.update(visible=False), gr.update(visible=True), gr.update(visible=True), gr.update(visible=True)\n",
        "\n",
        "        function_choice.change(\n",
        "            update_inputs_outputs,\n",
        "            inputs=[function_choice],\n",
        "            outputs=[input_image, input_word, top_k, output_text, output_image],\n",
        "        )\n",
        "\n",
        "        # 執行按鈕的點擊事件\n",
        "        def run_function(choice, image=None, word=None, top_k=3):\n",
        "            if choice == \"find_similar_words\":\n",
        "                return find_similar_words(image, top_k) , None , None\n",
        "            else:\n",
        "                image_path, result_text, img_base64 = find_image_for_word(word)\n",
        "                return result_text, image_path, img_base64\n",
        "\n",
        "        run_button.click(\n",
        "            run_function,\n",
        "            inputs=[function_choice, input_image, input_word, top_k],\n",
        "            outputs=[output_text, output_image, output_base64],\n",
        "        )\n",
        "\n",
        "    demo.launch(share=True, debug=True)\n",
        "\n",
        "launch_interface()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "collapsed_sections": [
        "FVMeX8_zaaA2",
        "C0yyLFeYw1bT"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}